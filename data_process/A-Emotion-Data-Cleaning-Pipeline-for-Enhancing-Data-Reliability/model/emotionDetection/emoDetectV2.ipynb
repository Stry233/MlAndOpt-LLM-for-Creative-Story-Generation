{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIH9NP0MZ6-O"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global var set\n",
    "import transformers\n",
    "# model info, change as needed\n",
    "batch_size = 16\n",
    "num_epochs = 16\n",
    "\n",
    "# model_checkpoint = \"bert-base-uncased\"\n",
    "# model_checkpoint = \"roberta-base\"\n",
    "# model_checkpoint = 'ProsusAI/finbert'\n",
    "model_checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "metric_name = \"f1\"\n",
    "# fileTag = \"original-plutchik-v1\"\n",
    "fileTag = 'clean-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Convert dataset to suitable format\n",
    "IMPORTANT: please never run this section again if you have your dataset ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "9a1aa9f2cc29473f9f8e5459d2641e76",
      "308fa6a7348140ec981a8d6c7d31f346",
      "8bc92587e35443488445e7521fbd0a13",
      "091f8220f33241f288faa0612853585f",
      "1045bb16e3694410898a73cf1b848917",
      "cb95e545fbdd4e99903bf634df694c9f",
      "5080d322a8034924b652b379c04667ed",
      "8b1899a0c4b144d7a5e6599f8afb8b65",
      "6111a73e684a47769bda7183a836ee91",
      "cd3570ddf67541d7818d97e236c54e54",
      "bbba60f793c14100934a268063f63d26"
     ]
    },
    "id": "sd1LiXGjZ420",
    "outputId": "1b5783cd-0e4e-4c92-c67c-57b9288f2381"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "trainDatasetOriginal = pd.read_csv(f'../../data/csv_version/dev/emotion/allcharlinepairs-{fileTag}.csv')\n",
    "testDatasetOriginal = pd.read_csv(f'../../data/csv_version/test/emotion/allcharlinepairs-{fileTag}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>storyid</th>\n",
       "      <th>linenum</th>\n",
       "      <th>char</th>\n",
       "      <th>emotionworkerid</th>\n",
       "      <th>context</th>\n",
       "      <th>sentence</th>\n",
       "      <th>affected</th>\n",
       "      <th>emotion</th>\n",
       "      <th>plutchik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>1</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>ann0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['joy', 'excited']</td>\n",
       "      <td>{'joy': 3, 'trust': 3, 'fear': 0, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>1</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>ann1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['content']</td>\n",
       "      <td>{'joy': 2, 'trust': 0, 'fear': 0, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>1</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>ann2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['hungry', 'anticipation']</td>\n",
       "      <td>{'joy': 2, 'trust': 0, 'fear': 0, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>2</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>ann1</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>I decided not to read a recipe since I've made...</td>\n",
       "      <td>yes</td>\n",
       "      <td>['excited']</td>\n",
       "      <td>{'joy': 3, 'trust': 3, 'fear': 0, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a2ddbb50-e45b-4ad3-becf-b2d8475172bf</td>\n",
       "      <td>2</td>\n",
       "      <td>I (myself)</td>\n",
       "      <td>ann2</td>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>I decided not to read a recipe since I've made...</td>\n",
       "      <td>yes</td>\n",
       "      <td>['confident']</td>\n",
       "      <td>{'joy': 2, 'trust': 3, 'fear': 0, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42949</th>\n",
       "      <td>42949</td>\n",
       "      <td>2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e</td>\n",
       "      <td>4</td>\n",
       "      <td>Marcus</td>\n",
       "      <td>ann1</td>\n",
       "      <td>Marcus was collecting shells on the beach.|He ...</td>\n",
       "      <td>Suddenly he felt a sharp pinch.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['sore']</td>\n",
       "      <td>{'joy': 2, 'trust': 2, 'fear': 2, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42950</th>\n",
       "      <td>42950</td>\n",
       "      <td>2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e</td>\n",
       "      <td>4</td>\n",
       "      <td>Marcus</td>\n",
       "      <td>ann2</td>\n",
       "      <td>Marcus was collecting shells on the beach.|He ...</td>\n",
       "      <td>Suddenly he felt a sharp pinch.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['surprised']</td>\n",
       "      <td>{'joy': 2, 'trust': 0, 'fear': 2, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42951</th>\n",
       "      <td>42951</td>\n",
       "      <td>2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e</td>\n",
       "      <td>5</td>\n",
       "      <td>Marcus</td>\n",
       "      <td>ann0</td>\n",
       "      <td>Marcus was collecting shells on the beach.|He ...</td>\n",
       "      <td>A crab was inside the shell pinching his leg.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['shocked']</td>\n",
       "      <td>{'joy': 0, 'trust': 0, 'fear': 2, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42952</th>\n",
       "      <td>42952</td>\n",
       "      <td>2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e</td>\n",
       "      <td>5</td>\n",
       "      <td>Marcus</td>\n",
       "      <td>ann1</td>\n",
       "      <td>Marcus was collecting shells on the beach.|He ...</td>\n",
       "      <td>A crab was inside the shell pinching his leg.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['surprised']</td>\n",
       "      <td>{'joy': 0, 'trust': 0, 'fear': 2, 'surprise': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42953</th>\n",
       "      <td>42953</td>\n",
       "      <td>2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e</td>\n",
       "      <td>5</td>\n",
       "      <td>Marcus</td>\n",
       "      <td>ann2</td>\n",
       "      <td>Marcus was collecting shells on the beach.|He ...</td>\n",
       "      <td>A crab was inside the shell pinching his leg.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['pained']</td>\n",
       "      <td>{'joy': 0, 'trust': 0, 'fear': 3, 'surprise': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42954 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                               storyid  linenum        char  \\\n",
       "0               0  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        1  I (myself)   \n",
       "1               1  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        1  I (myself)   \n",
       "2               2  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        1  I (myself)   \n",
       "3               3  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        2  I (myself)   \n",
       "4               4  a2ddbb50-e45b-4ad3-becf-b2d8475172bf        2  I (myself)   \n",
       "...           ...                                   ...      ...         ...   \n",
       "42949       42949  2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e        4      Marcus   \n",
       "42950       42950  2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e        4      Marcus   \n",
       "42951       42951  2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e        5      Marcus   \n",
       "42952       42952  2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e        5      Marcus   \n",
       "42953       42953  2aa1aca3-9264-4e27-9ebb-fa5de8e7e84e        5      Marcus   \n",
       "\n",
       "      emotionworkerid                                            context  \\\n",
       "0                ann0                                                NaN   \n",
       "1                ann1                                                NaN   \n",
       "2                ann2                                                NaN   \n",
       "3                ann1  I began making fish curry for my boyfriend and I.   \n",
       "4                ann2  I began making fish curry for my boyfriend and I.   \n",
       "...               ...                                                ...   \n",
       "42949            ann1  Marcus was collecting shells on the beach.|He ...   \n",
       "42950            ann2  Marcus was collecting shells on the beach.|He ...   \n",
       "42951            ann0  Marcus was collecting shells on the beach.|He ...   \n",
       "42952            ann1  Marcus was collecting shells on the beach.|He ...   \n",
       "42953            ann2  Marcus was collecting shells on the beach.|He ...   \n",
       "\n",
       "                                                sentence affected  \\\n",
       "0      I began making fish curry for my boyfriend and I.      yes   \n",
       "1      I began making fish curry for my boyfriend and I.      yes   \n",
       "2      I began making fish curry for my boyfriend and I.      yes   \n",
       "3      I decided not to read a recipe since I've made...      yes   \n",
       "4      I decided not to read a recipe since I've made...      yes   \n",
       "...                                                  ...      ...   \n",
       "42949                    Suddenly he felt a sharp pinch.      yes   \n",
       "42950                    Suddenly he felt a sharp pinch.      yes   \n",
       "42951      A crab was inside the shell pinching his leg.      yes   \n",
       "42952      A crab was inside the shell pinching his leg.      yes   \n",
       "42953      A crab was inside the shell pinching his leg.      yes   \n",
       "\n",
       "                          emotion  \\\n",
       "0              ['joy', 'excited']   \n",
       "1                     ['content']   \n",
       "2      ['hungry', 'anticipation']   \n",
       "3                     ['excited']   \n",
       "4                   ['confident']   \n",
       "...                           ...   \n",
       "42949                    ['sore']   \n",
       "42950               ['surprised']   \n",
       "42951                 ['shocked']   \n",
       "42952               ['surprised']   \n",
       "42953                  ['pained']   \n",
       "\n",
       "                                                plutchik  \n",
       "0      {'joy': 3, 'trust': 3, 'fear': 0, 'surprise': ...  \n",
       "1      {'joy': 2, 'trust': 0, 'fear': 0, 'surprise': ...  \n",
       "2      {'joy': 2, 'trust': 0, 'fear': 0, 'surprise': ...  \n",
       "3      {'joy': 3, 'trust': 3, 'fear': 0, 'surprise': ...  \n",
       "4      {'joy': 2, 'trust': 3, 'fear': 0, 'surprise': ...  \n",
       "...                                                  ...  \n",
       "42949  {'joy': 2, 'trust': 2, 'fear': 2, 'surprise': ...  \n",
       "42950  {'joy': 2, 'trust': 0, 'fear': 2, 'surprise': ...  \n",
       "42951  {'joy': 0, 'trust': 0, 'fear': 2, 'surprise': ...  \n",
       "42952  {'joy': 0, 'trust': 0, 'fear': 2, 'surprise': ...  \n",
       "42953  {'joy': 0, 'trust': 0, 'fear': 3, 'surprise': ...  \n",
       "\n",
       "[42954 rows x 10 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDatasetOriginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "trainDatasetProcessed = DataFrame({'sentence' : trainDatasetOriginal['sentence'], \n",
    "                                   'joy'  : [0 if (ast.literal_eval(trainDatasetOriginal['plutchik'][index])['joy'] == 0) else 1 for index in range(trainDatasetOriginal.shape[0])], \n",
    "                                   'trust' : [0 if (ast.literal_eval(trainDatasetOriginal['plutchik'][index])['trust'] == 0) else 1 for index in range(trainDatasetOriginal.shape[0])], \n",
    "                                   'fear'  : [0 if (ast.literal_eval(trainDatasetOriginal['plutchik'][index])['fear'] == 0) else 1 for index in range(trainDatasetOriginal.shape[0])],\n",
    "                                   'surprise': [0 if (ast.literal_eval(trainDatasetOriginal['plutchik'][index])['surprise'] == 0) else 1 for index in range(trainDatasetOriginal.shape[0])],\n",
    "                                   'sadness': [0 if (ast.literal_eval(trainDatasetOriginal['plutchik'][index])['sadness'] == 0) else 1 for index in range(trainDatasetOriginal.shape[0])],\n",
    "                                   'disgust': [0 if (ast.literal_eval(trainDatasetOriginal['plutchik'][index])['disgust'] == 0) else 1 for index in range(trainDatasetOriginal.shape[0])],\n",
    "                                   'anger': [0 if (ast.literal_eval(trainDatasetOriginal['plutchik'][index])['anger'] == 0) else 1 for index in range(trainDatasetOriginal.shape[0])],\n",
    "                                   'anticipation': [0 if (ast.literal_eval(trainDatasetOriginal['plutchik'][index])['anticipation'] == 0) else 1 for index in range(trainDatasetOriginal.shape[0])]})\n",
    "testDatasetProcessed = DataFrame({'sentence' : testDatasetOriginal['sentence'], \n",
    "                                   'joy'  : [0 if (ast.literal_eval(testDatasetOriginal['plutchik'][index])['joy'] == 0) else 1 for index in range(testDatasetOriginal.shape[0])], \n",
    "                                   'trust' : [0 if (ast.literal_eval(testDatasetOriginal['plutchik'][index])['trust'] == 0) else 1 for index in range(testDatasetOriginal.shape[0])], \n",
    "                                   'fear'  : [0 if (ast.literal_eval(testDatasetOriginal['plutchik'][index])['fear'] == 0) else 1 for index in range(testDatasetOriginal.shape[0])],\n",
    "                                   'surprise': [0 if (ast.literal_eval(testDatasetOriginal['plutchik'][index])['surprise'] == 0) else 1 for index in range(testDatasetOriginal.shape[0])],\n",
    "                                   'sadness': [0 if (ast.literal_eval(testDatasetOriginal['plutchik'][index])['sadness'] == 0) else 1 for index in range(testDatasetOriginal.shape[0])],\n",
    "                                   'disgust': [0 if (ast.literal_eval(testDatasetOriginal['plutchik'][index])['disgust'] == 0) else 1 for index in range(testDatasetOriginal.shape[0])],\n",
    "                                   'anger': [0 if (ast.literal_eval(testDatasetOriginal['plutchik'][index])['anger'] == 0) else 1 for index in range(testDatasetOriginal.shape[0])],\n",
    "                                   'anticipation': [0 if (ast.literal_eval(testDatasetOriginal['plutchik'][index])['anticipation'] == 0) else 1 for index in range(testDatasetOriginal.shape[0])]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>joy</th>\n",
       "      <th>trust</th>\n",
       "      <th>fear</th>\n",
       "      <th>surprise</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I began making fish curry for my boyfriend and I.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I decided not to read a recipe since I've made...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I decided not to read a recipe since I've made...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42949</th>\n",
       "      <td>Suddenly he felt a sharp pinch.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42950</th>\n",
       "      <td>Suddenly he felt a sharp pinch.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42951</th>\n",
       "      <td>A crab was inside the shell pinching his leg.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42952</th>\n",
       "      <td>A crab was inside the shell pinching his leg.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42953</th>\n",
       "      <td>A crab was inside the shell pinching his leg.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42954 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  joy  trust  fear  \\\n",
       "0      I began making fish curry for my boyfriend and I.    1      1     0   \n",
       "1      I began making fish curry for my boyfriend and I.    1      0     0   \n",
       "2      I began making fish curry for my boyfriend and I.    1      0     0   \n",
       "3      I decided not to read a recipe since I've made...    1      1     0   \n",
       "4      I decided not to read a recipe since I've made...    1      1     0   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "42949                    Suddenly he felt a sharp pinch.    1      1     1   \n",
       "42950                    Suddenly he felt a sharp pinch.    1      0     1   \n",
       "42951      A crab was inside the shell pinching his leg.    0      0     1   \n",
       "42952      A crab was inside the shell pinching his leg.    0      0     1   \n",
       "42953      A crab was inside the shell pinching his leg.    0      0     1   \n",
       "\n",
       "       surprise  sadness  disgust  anger  anticipation  \n",
       "0             1        0        0      0             1  \n",
       "1             0        0        0      0             0  \n",
       "2             0        0        0      0             1  \n",
       "3             0        0        0      0             1  \n",
       "4             0        0        0      0             1  \n",
       "...         ...      ...      ...    ...           ...  \n",
       "42949         0        1        1      1             1  \n",
       "42950         1        0        0      0             1  \n",
       "42951         1        0        0      0             1  \n",
       "42952         1        0        0      0             1  \n",
       "42953         1        0        0      0             1  \n",
       "\n",
       "[42954 rows x 9 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDatasetProcessed.to_csv(f'./dataset/emoDetect-{fileTag}-train.csv')\n",
    "trainDatasetProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>joy</th>\n",
       "      <th>trust</th>\n",
       "      <th>fear</th>\n",
       "      <th>surprise</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A cook was carrying an armful of oranged in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A cook was carrying an armful of oranged in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He dropped one on the floor by accident.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He dropped one on the floor by accident.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He dropped one on the floor by accident.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43190</th>\n",
       "      <td>She had been dieting for weeks and was starving.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43191</th>\n",
       "      <td>After she stuffed herself, she regretted it.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43192</th>\n",
       "      <td>After she stuffed herself, she regretted it.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43193</th>\n",
       "      <td>Rosemary swore to eat healthier tomorrow.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43194</th>\n",
       "      <td>Rosemary swore to eat healthier tomorrow.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43195 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  joy  trust  fear  \\\n",
       "0      A cook was carrying an armful of oranged in th...    1      1     0   \n",
       "1      A cook was carrying an armful of oranged in th...    1      0     0   \n",
       "2               He dropped one on the floor by accident.    0      0     1   \n",
       "3               He dropped one on the floor by accident.    0      0     0   \n",
       "4               He dropped one on the floor by accident.    0      0     0   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "43190   She had been dieting for weeks and was starving.    1      1     0   \n",
       "43191       After she stuffed herself, she regretted it.    0      0     0   \n",
       "43192       After she stuffed herself, she regretted it.    0      0     0   \n",
       "43193          Rosemary swore to eat healthier tomorrow.    0      0     0   \n",
       "43194          Rosemary swore to eat healthier tomorrow.    1      1     0   \n",
       "\n",
       "       surprise  sadness  disgust  anger  anticipation  \n",
       "0             1        0        0      0             1  \n",
       "1             0        0        0      0             1  \n",
       "2             1        0        0      0             1  \n",
       "3             0        0        1      1             0  \n",
       "4             1        0        0      1             0  \n",
       "...         ...      ...      ...    ...           ...  \n",
       "43190         0        0        0      0             1  \n",
       "43191         0        0        1      0             0  \n",
       "43192         0        1        0      0             0  \n",
       "43193         0        1        1      1             0  \n",
       "43194         0        0        0      0             1  \n",
       "\n",
       "[43195 rows x 9 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDatasetProcessed.to_csv(f'./dataset/emoDetect-{fileTag}-test.csv')\n",
    "testDatasetProcessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCL02vQgxYTO"
   },
   "source": [
    "# Start convert and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRd1kXQZjYIY",
    "outputId": "8021590c-5607-4a9c-a474-bc57da503c93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3e6657a132cbe82c\n",
      "Reusing dataset csv (C:\\Users\\JAM_0\\.cache\\huggingface\\datasets\\csv\\default-3e6657a132cbe82c\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443a06d8fa1547a983809ce3357fe71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'sentence', 'joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation'],\n",
       "        num_rows: 11610\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'sentence', 'joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation'],\n",
       "        num_rows: 11129\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "dataset = load_dataset('csv', data_files={'train': f'./dataset/emoDetect-{fileTag}-train.csv', \n",
    "                                           'test': f'./dataset/emoDetect-{fileTag}-test.csv'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgS0wMWExcqP"
   },
   "source": [
    "Let's check the first example of the training split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unjuTtKUjZI3",
    "outputId": "6f1e5051-8272-40f9-ced8-ba17a105e904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'sentence': 'I began making fish curry for my boyfriend and I.',\n",
       " 'joy': 1,\n",
       " 'trust': 1,\n",
       " 'fear': 0,\n",
       " 'surprise': 1,\n",
       " 'sadness': 0,\n",
       " 'disgust': 0,\n",
       " 'anger': 0,\n",
       " 'anticipation': 1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DV0Rtetxgd4"
   },
   "source": [
    "The dataset consists of tweets, labeled with one or more emotions. \n",
    "\n",
    "Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5vZhQpvkE8s",
    "outputId": "5d513b30-f209-492f-c6ab-245d64a67d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joy',\n",
       " 'trust',\n",
       " 'fear',\n",
       " 'surprise',\n",
       " 'sadness',\n",
       " 'disgust',\n",
       " 'anger',\n",
       " 'anticipation']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['Unnamed: 0', 'sentence']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ3Teyjmank2"
   },
   "source": [
    "## Preprocess data\n",
    "\n",
    "As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
    "\n",
    "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' `BCEWithLogitsLoss` (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "AFWlSsbZaRLc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json from cache at C:\\Users\\JAM_0/.cache\\huggingface\\transformers\\4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/vocab.txt from cache at C:\\Users\\JAM_0/.cache\\huggingface\\transformers\\83261b0c74c462e53d6367de0646b1fca07d0f15f1be045156b9cf8c71279cc9.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/tokenizer_config.json from cache at C:\\Users\\JAM_0/.cache\\huggingface\\transformers\\d44ec0488a5f13d92b3934cb68cc5849bd74ce63ede2eea2bf3c675e1e57297c.627f9558061e7bc67ed0f516b2f7efc1351772cc8553101f08748d44aada8b11\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json from cache at C:\\Users\\JAM_0/.cache\\huggingface\\transformers\\4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json from cache at C:\\Users\\JAM_0/.cache\\huggingface\\transformers\\4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"sentence\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4ENBTdulBEI",
    "outputId": "02554a1f-4961-461a-bf29-555b8debeabf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\JAM_0\\.cache\\huggingface\\datasets\\csv\\default-3e6657a132cbe82c\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-b8f84e6f4a47c3f8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\JAM_0\\.cache\\huggingface\\datasets\\csv\\default-3e6657a132cbe82c\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-7d7653b6f5d9c429.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0enAb0W9o25W",
    "outputId": "55bc5ba6-d169-49c6-f562-bb7ea4143866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "D0McCtJ8HRJY",
    "outputId": "82fb0336-51a3-40ad-ebc0-65eeb7cf4b6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i began making fish curry for my boyfriend and i. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdIvj6WjHeZQ",
    "outputId": "418c14d2-cca3-44e9-d0a2-6ad4ca7a007c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4Dx95t2o6N9",
    "outputId": "3ce6c923-0b45-4743-bc4b-7771d088b03e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joy', 'trust', 'surprise', 'anticipation']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgpKXDfvKBxn"
   },
   "source": [
    "Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Lk6Cq9duKBkA"
   },
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5qSmCgWefWs"
   },
   "source": [
    "## Define model\n",
    "\n",
    "Here we define a model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top. One should fine-tune this head, together with the pre-trained base on a labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XPL1Z_RegBF",
    "outputId": "22994300-8c93-421e-faa4-678d6cc14aab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json from cache at C:\\Users\\JAM_0/.cache\\huggingface\\transformers\\4e60bb8efad3d4b7dc9969bf204947c185166a0a3cf37ddb6f481a876a3777b5.9f8326d0b7697c7fd57366cdde57032f46bc10e37ae81cb7eb564d66d23ec96b\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"joy\",\n",
      "    \"1\": \"trust\",\n",
      "    \"2\": \"fear\",\n",
      "    \"3\": \"surprise\",\n",
      "    \"4\": \"sadness\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"anger\",\n",
      "    \"7\": \"anticipation\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 6,\n",
      "    \"anticipation\": 7,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 2,\n",
      "    \"joy\": 0,\n",
      "    \"sadness\": 4,\n",
      "    \"surprise\": 3,\n",
      "    \"trust\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/pytorch_model.bin from cache at C:\\Users\\JAM_0/.cache\\huggingface\\transformers\\8d04c767d9d4c14d929ce7ad8e067b80c74dbdb212ef4c3fb743db4ee109fae0.9d268a35da669ead745c44d369dc9948b408da5010c6bac414414a7e33d5748c\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id, \n",
    "                                                           ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjJGEXShp7te"
   },
   "source": [
    "## Train the model!\n",
    "\n",
    "We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things: \n",
    "\n",
    "* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.\n",
    "* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "dR2GmpvDqbuZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    # learning_rate=9e-7,\n",
    "    learning_rate=6e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_v2fPFFJ3-v"
   },
   "source": [
    "We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "797b2WHJqUgZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxNo4_TsvzDm"
   },
   "source": [
    "Let's verify a batch as well as a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IlOgGiojuWwG",
    "outputId": "cd0b2c99-b520-468d-8ffc-c36211b7820a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y41Kre_jvD7x",
    "outputId": "b6ca888b-6371-40fb-ab83-3dc24d28320a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1045,  2211,  2437,  3869, 15478,  2005,  2026,  6898,  1998,\n",
       "         1045,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxWcnZ8ku12V",
    "outputId": "26522911-c3cd-466a-ae2d-d81d23003c23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6971, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[ 0.3067, -0.0404,  0.0743, -0.0565,  0.2295, -0.1519,  0.2057,  0.1446]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-X2brZcv0X6"
   },
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "chq_3nUz73ib"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KXmFds8js6P8",
    "outputId": "66ebb2ab-f93f-48aa-a4dc-36f55f2f8559"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\miniconda3\\envs\\pytorchEnvWithDataSci\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11610\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11616' max='11616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11616/11616 18:23, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.513809</td>\n",
       "      <td>0.687942</td>\n",
       "      <td>0.732518</td>\n",
       "      <td>0.149340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.517700</td>\n",
       "      <td>0.498210</td>\n",
       "      <td>0.702404</td>\n",
       "      <td>0.745091</td>\n",
       "      <td>0.169647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.493014</td>\n",
       "      <td>0.707881</td>\n",
       "      <td>0.750143</td>\n",
       "      <td>0.178453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>0.495391</td>\n",
       "      <td>0.708241</td>\n",
       "      <td>0.749431</td>\n",
       "      <td>0.179621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.435600</td>\n",
       "      <td>0.497070</td>\n",
       "      <td>0.708379</td>\n",
       "      <td>0.751167</td>\n",
       "      <td>0.184114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.504051</td>\n",
       "      <td>0.708572</td>\n",
       "      <td>0.750299</td>\n",
       "      <td>0.182676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.407400</td>\n",
       "      <td>0.510026</td>\n",
       "      <td>0.707999</td>\n",
       "      <td>0.750787</td>\n",
       "      <td>0.183664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>0.512795</td>\n",
       "      <td>0.707875</td>\n",
       "      <td>0.750188</td>\n",
       "      <td>0.183754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>0.516743</td>\n",
       "      <td>0.703349</td>\n",
       "      <td>0.747136</td>\n",
       "      <td>0.181777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.524007</td>\n",
       "      <td>0.705579</td>\n",
       "      <td>0.748251</td>\n",
       "      <td>0.180340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.375500</td>\n",
       "      <td>0.526451</td>\n",
       "      <td>0.702531</td>\n",
       "      <td>0.745509</td>\n",
       "      <td>0.177644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.529537</td>\n",
       "      <td>0.704547</td>\n",
       "      <td>0.747189</td>\n",
       "      <td>0.180430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.362800</td>\n",
       "      <td>0.529769</td>\n",
       "      <td>0.703235</td>\n",
       "      <td>0.747214</td>\n",
       "      <td>0.179261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>0.534498</td>\n",
       "      <td>0.700661</td>\n",
       "      <td>0.744429</td>\n",
       "      <td>0.177734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.356500</td>\n",
       "      <td>0.535308</td>\n",
       "      <td>0.700913</td>\n",
       "      <td>0.744811</td>\n",
       "      <td>0.178093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.536269</td>\n",
       "      <td>0.702308</td>\n",
       "      <td>0.745861</td>\n",
       "      <td>0.178992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-726\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-726\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-726\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-726\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-726\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-1452\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-1452\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-1452\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-1452\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-1452\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-2178\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-2178\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-2178\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-2178\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-2178\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-2904\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-2904\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-2904\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-2904\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-2904\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-3630\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-3630\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-3630\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-3630\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-3630\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-4356\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-4356\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-4356\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-4356\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-4356\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-5082\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-5082\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-5082\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-5082\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-5082\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-5808\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-5808\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-5808\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-5808\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-5808\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-6534\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-6534\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-6534\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-6534\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-6534\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-7260\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-7260\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-7260\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-7260\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-7260\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-7986\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-7986\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-7986\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-7986\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-7986\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-8712\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-8712\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-8712\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-8712\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-8712\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-9438\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-9438\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-9438\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-9438\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-9438\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-10164\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-10164\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-10164\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-10164\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-10164\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-10890\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-10890\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-10890\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-10890\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-10890\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11129\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english\\checkpoint-11616\n",
      "Configuration saved in bert-finetuned-sem_eval-english\\checkpoint-11616\\config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english\\checkpoint-11616\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english\\checkpoint-11616\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english\\checkpoint-11616\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-sem_eval-english\\checkpoint-4356 (score: 0.7085721868365181).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11616, training_loss=0.41031546599280405, metrics={'train_runtime': 1103.9228, 'train_samples_per_second': 168.273, 'train_steps_per_second': 10.522, 'total_flos': 6152444243804160.0, 'train_loss': 0.41031546599280405, 'epoch': 16.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataLog = pd.DataFrame(trainer.state.log_history)\n",
    "dataLog.to_csv(f'./trainingMetric/TI-{model_checkpoint}-{fileTag}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nmvJp0pLq-3"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Let's test the model on a new sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fxjfr8PLD42"
   },
   "outputs": [],
   "source": [
    "text = \"I'm happy I can finally train a model for multi-label classification\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8THm5-XgNHPm"
   },
   "source": [
    "The logits that come out of the model are of shape (batch_size, num_labels). As we are only forwarding a single sentence through the model, the `batch_size` equals 1. The logits is a tensor that contains the (unnormalized) scores for every individual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOBosj4UL2tU",
    "outputId": "be370f49-3840-4c76-b193-76083e49701e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC4XdDaHNVcd"
   },
   "source": [
    "To turn them into actual predicted labels, we first apply a sigmoid function independently to every score, such that every score is turned into a number between 0 and 1, that can be interpreted as a \"probability\" for how certain the model is that a given class belongs to the input text.\n",
    "\n",
    "Next, we use a threshold (typically, 0.5) to turn every probability into either a 1 (which means, we predict the label for the given example) or a 0 (which means, we don't predict the label for the given example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEkAQleMMT0k",
    "outputId": "fddb51cb-bf01-420a-fd5b-4a7c2e775446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy', 'optimism']\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fine-tuning BERT (and friends) for multi-label text classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorchEnvWithDataSci",
   "language": "python",
   "name": "pytorchenvwithdatasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "091f8220f33241f288faa0612853585f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6111a73e684a47769bda7183a836ee91",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b1899a0c4b144d7a5e6599f8afb8b65",
      "value": 3
     }
    },
    "1045bb16e3694410898a73cf1b848917": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbba60f793c14100934a268063f63d26",
      "placeholder": "​",
      "style": "IPY_MODEL_cd3570ddf67541d7818d97e236c54e54",
      "value": " 3/3 [00:00&lt;00:00, 75.93it/s]"
     }
    },
    "308fa6a7348140ec981a8d6c7d31f346": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5080d322a8034924b652b379c04667ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6111a73e684a47769bda7183a836ee91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b1899a0c4b144d7a5e6599f8afb8b65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8bc92587e35443488445e7521fbd0a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5080d322a8034924b652b379c04667ed",
      "placeholder": "​",
      "style": "IPY_MODEL_cb95e545fbdd4e99903bf634df694c9f",
      "value": "100%"
     }
    },
    "9a1aa9f2cc29473f9f8e5459d2641e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bc92587e35443488445e7521fbd0a13",
       "IPY_MODEL_091f8220f33241f288faa0612853585f",
       "IPY_MODEL_1045bb16e3694410898a73cf1b848917"
      ],
      "layout": "IPY_MODEL_308fa6a7348140ec981a8d6c7d31f346"
     }
    },
    "bbba60f793c14100934a268063f63d26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb95e545fbdd4e99903bf634df694c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd3570ddf67541d7818d97e236c54e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
